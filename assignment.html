<html>
<h1>
My essay for Rhet 110 last semester was about how AI is being used and how AI can sometimes turn out racist, among other things. AI tends to emulate behaviors that humans produce on a large scale, so over the past bunch of years, we’ve seen AI identify foods by looking at the race of the person holding it, hold biases against people of differing sexualities, and perform objectively worse when it thinks the user is black. 

I’ve been coding since 3rd grade, and I’ve weirdly grown up with AI the same way a normal kid might grow up with baseball or dance. Recently, I joined a research project in this area, specifically how AI performs in different languages and cultural contexts. In writing this essay, I was attempting to at minimum inform my professor of the risks of AI. Any established user of ChatGPT or Anthropic’s Claude might’ve noticed that the platforms tend to be quite left-leaning. I think people like to see public-facing institutions be moderately left-leaning, it just generally feels safer and more proper. I must admit, it would be pretty jarring if OpenAI or Google were conservative and supported pro-life movements. Though this left-leaning persona is just that: a facade. AI systems are trained on billions of web pages. It is entirely unreasonable to expect that after teaching an AI all of that information, that it would somehow have this unified value code that meaningfully aligned with that of its creators. While LLMs might appear to be left-leaning, they are quite randomly and unexpectedly harborous of conservative biases. 

AI was quick onset. No one knew about AI when I was in 3rd grade, and no one knew because it was terrible. Ask a question about cooking times on lasagna? It might output a verbatim Grimm fairy tale. When AI really started to become a household topic, it really caught on quite fast, nobody knew just exactly how it worked or what to expect, and I’d confidently assert that no one – even the creators – understand it even now. 

I wanted to write my final essay on this because it was something personally quite interesting to me, and the thought that so many people didn’t know the legitimate risks that AI poses was equally distressing to me. 

I suppose my first audience was my professor. She was, after all, the only one I knew that was going to read it. Critically, she was also the one that graded the damn thing and I wasn’t in a hurry to get a bad grade in the class.

Secondly and thirdly, my audience might blend a bit more together. On a more imaginative level, I wasn’t writing it for academics or researchers, but more so for people in my life that I reckoned should know a bit about it. I wrote it to my sister, my moms, my uncle, but also my friends, teachers, boss, and all the people in my life that aren’t incredibly well-informed on the issue. I, of course, knew that they wouldn’t literally be reading it, but it helped me understand what type of person I was appealing to. I wanted to write my paper in non-research terms, the only terms I defined were ‘AI’ and ‘LLM,’ both acronyms not so far outside general knowledge. I wanted my paper to feel approachable, friendly, and crucially, easy to understand. I wasn’t writing the paper to prove I was smarter than everyone else, but because I had recently seen quite a bit of information that convinced me we should all be terrified. 

In the research papers I contribute to on AI, people write like they’re trying to prove that they’re more sophisticated than everyone else around them. It’s absurd how the most interesting frontier information of our society is locked behind fancy terminology. A rebuttal might be that the terminology is domain-specific, and is meant to be increasingly precise, but with all the brain power authors of academic writing have, you’d think they might bother to democratize their findings. Academics often are dismayed at how few people understand their work, while simultaneously doing everything in their power to keep it opaque. 

I’m a business major, and I also think about how writing is similarly complex in financial reports. Oddly though, I find it more justified in financial writing. I like to read the Financial Times or the Wall Street Journal, and the complex terminology that is used I feel is less so about stroking the ego of the author, and more genuinely reflective of the increasingly intricate financial systems that power our world.

</h1>
</html>